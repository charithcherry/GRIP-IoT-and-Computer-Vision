{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1 : Object Detection \n",
    "\n",
    "                                                                                         GRIP    : The Sparks Foundation\n",
    "\n",
    "                                                                                         Function: Iot and Computer Vision\n",
    "\n",
    "                                                                                         Batch   : January 2021\n",
    "                                                                          \n",
    "                                                                                         Author  : Charith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Object Detection\n",
    "\n",
    "Object detection, a subset of computer vision, is an automated method for locating interesting objects in an image with respect to the background.\n",
    "\n",
    "Solving the object detection problem means placing a tight bounding box around these objects and associating the correct object category with each bounding box. Like other computer vision tasks, deep learning is the state-of-art method to perform object detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet\n",
    "\n",
    "MobileNet is an efficient and portable CNN architecture that is used in real world applications. MobileNets primarily use depthwise seperable convolutions in place of the standard convolutions used in earlier architectures to build lighter models.MobileNets introduce two new global hyperparameters(width multiplier and resolution multiplier) that allow model developers to trade off latency or accuracy for speed and low size depending on their requirements.\n",
    "\n",
    "For more information : https://iq.opengenus.org/mobilenet-v1-architecture/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Shot Detector\n",
    "\n",
    "\n",
    "Single Shot detector like YOLO takes only one shot to detect multiple objects present in an image using multibox.\n",
    "\n",
    "It is significantly faster in speed and high-accuracy object detection algorithm. A quick comparison between speed and accuracy of different object detection models on VOC2007\n",
    "\n",
    "For more info: https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining MobileNets and Single Shot Detectors for fast, efficient deep-learning based object detection\n",
    "\n",
    "we will use the MobileNet SSD + deep neural network (dnn ) module in OpenCV to build our object detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING NECESSARY PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect, then generate a set of bounding box colors for each class\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] loading done\n"
     ]
    }
   ],
   "source": [
    "# load our serialized model from disk\n",
    "#loading a pre-trained Caffe network.\n",
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe('MobileNetSSD_deployprototxt.txt','MobileNetSSD_deploy.caffemodel')\n",
    "print(\"[INFO] loading done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Image for Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('images/mix2.jpg')\n",
    "image= cv2.resize(image, (500, 500))\n",
    "cv2.imshow(\"detected\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the height and width (Line 35), and calculate a 300 by 300 pixel blob  from our image\n",
    "(h, w) = image.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(image, 0.007843,(500, 500), 127.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] computing object detections...\n"
     ]
    }
   ],
   "source": [
    "# weâ€™ll pass this blob through the neural network\n",
    "print(\"[INFO] computing object detections...\")\n",
    "net.setInput(blob)\n",
    "detections = net.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After computing the detections,we next label the predictions, boxing the objects and displaying the probability  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] bus: 61.35%\n",
      "[INFO] car: 95.98%\n",
      "[INFO] car: 87.62%\n",
      "[INFO] car: 84.55%\n",
      "[INFO] car: 68.32%\n",
      "[INFO] person: 90.17%\n",
      "[INFO] sofa: 43.87%\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0, detections.shape[2]):\n",
    "# extract the confidence (i.e., probability) associated with the\n",
    "# prediction\n",
    "   confidence = detections[0, 0, i, 2]\n",
    "# filter out weak detections by ensuring the `confidence` is\n",
    "# greater than the minimum confidence\n",
    "   if confidence > 0.3 :\n",
    "   # extract the index of the class label from the `detections`,\n",
    "   # then compute the (x, y)-coordinates of the bounding box for\n",
    "   # the object\n",
    "        idx = int(detections[0, 0, i, 1])\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "        # display the prediction\n",
    "        label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
    "        print(\"[INFO] {}\".format(label))\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY),COLORS[idx], 2)\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "        cv2.putText(image, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the output image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Function for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(imageurl):\n",
    "    image = cv2.imread(\"images/\"+imageurl)\n",
    "    image= cv2.resize(image, (300, 300))\n",
    "    cv2.imshow(\"car\", image)\n",
    "    cv2.waitKey(0)\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.007843,(200, 200), 127.5)\n",
    "    print(\"[INFO] computing object detections...\")\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > 0.41:\n",
    "            # extract the index of the class label from the `detections`,\n",
    "            # then compute the (x, y)-coordinates of the bounding box for\n",
    "            # the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            # display the prediction\n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[idx], confidence * 100)\n",
    "            print(\"[INFO] {}\".format(label))\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY),COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(image, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "        \n",
    "     # show the output image\n",
    "    cv2.imshow(\"Output\", image)\n",
    "    cv2.waitKey(0)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] computing object detections...\n",
      "[INFO] car: 99.47%\n",
      "[INFO] car: 95.94%\n"
     ]
    }
   ],
   "source": [
    "detect('2cars.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] computing object detections...\n",
      "[INFO] cow: 79.56%\n",
      "[INFO] horse: 87.98%\n",
      "[INFO] sheep: 85.84%\n"
     ]
    }
   ],
   "source": [
    "detect('doghorsesheep.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] computing object detections...\n",
      "[INFO] dog: 89.87%\n",
      "[INFO] person: 90.36%\n"
     ]
    }
   ],
   "source": [
    "detect('dogperson2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] computing object detections...\n",
      "[INFO] horse: 99.86%\n",
      "[INFO] horse: 78.51%\n"
     ]
    }
   ],
   "source": [
    "detect('horse.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
